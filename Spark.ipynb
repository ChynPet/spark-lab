{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import datetime as dt\n",
    "import time\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import current_timestamp, from_unixtime\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_now():\n",
    "    return round(datetime.utcnow().timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_users(dataset, n_client=200):\n",
    "\n",
    "    id_start = 0\n",
    "    \n",
    "    if dataset != None:\n",
    "        id_start = dataset.agg({'id': 'max'}).collect()[0]['max(id)'] + 1\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(n_client):\n",
    "        firstName = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        lastName = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        row = (id_start, firstName, lastName)\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "    \n",
    "    df = spark.createDataFrame(rows, schema='Id bigint, FirstName string, LastName string')\n",
    "    \n",
    "    if dataset != None:\n",
    "        df = DataFrame.unionAll(dataset, df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_users(None, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('spark/client.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = spark.read.parquet('spark/client.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_drivers(dataset, n_client=200):\n",
    "    id_start = 0\n",
    "    \n",
    "    if dataset != None:\n",
    "        id_start = dataset.agg({'id': 'max'}).collect()[0]['max(id)'] + 1\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(n_client):\n",
    "        firstName = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        lastName = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        car = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        row = (id_start, firstName, lastName, car)\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "        \n",
    "    df = spark.createDataFrame(rows, schema='Id bigint, FirstName string, LastName string, Car string')\n",
    "    \n",
    "    if dataset != None:\n",
    "        df = DataFrame.unionAll(dataset, df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_drivers(None, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('spark/driver.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = spark.read.parquet('spark/driver.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_address = spark.read.csv('spark/london-address.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_orders(clients, drivers, london_address, orders, n_count=50):\n",
    "    ids_clients = clients.select('Id').collect()\n",
    "    ids_drivers = drivers.select('Id').collect()\n",
    "    coordinates = london_address.select('oseast1m', 'osnrth1m').collect()\n",
    "    grids = [{'oseast1m': int(i['oseast1m']),'osnrth1m': int(i['osnrth1m'])} for i in coordinates]\n",
    "    \n",
    "    id_start = 0\n",
    "    \n",
    "    if orders != None:\n",
    "        id_start = orders.agg({'id': 'max'}).collect()[0]['max(id)'] + 1\n",
    "    rows = []\n",
    "    \n",
    "    for i in range(n_count):\n",
    "            \n",
    "        client_id = random.choice(ids_clients)\n",
    "        driver_id = random.choice(ids_drivers)\n",
    "        ids_clients.remove(client_id)\n",
    "        ids_drivers.remove(driver_id)\n",
    "        price = random.randint(0, 1500)\n",
    "        created = datetime.utcnow()\n",
    "        start = random.choice(grids)\n",
    "        end = random.choice(grids)\n",
    "        \n",
    "        while end['oseast1m'] == start['oseast1m'] and end['osnrth1m'] == start['osnrth1m']:\n",
    "            end = random.choice(grids)\n",
    "            \n",
    "        row = (id_start, str(start), str(end), client_id['Id'], driver_id['Id'], price, False, created, created)\n",
    "\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "        \n",
    "    df = spark.createDataFrame(rows, schema='Id bigint, Start string, End string, ClientId bigint, DriverId bigint, Price bigint, Closed boolean, Created timestamp, Modified timestamp')\n",
    "    \n",
    "    if orders != None:\n",
    "        df = DataFrame.unionAll(orders, df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = create_orders(clients, drivers, london_address, None, n_count=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.parquet('spark/orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.read.parquet('spark/orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_movement(drivers, orders, london_address, movement, n_count_order=125000, n_count_worde=100000):\n",
    "    ID = 'Id'\n",
    "    DRIVER_ID = 'DriverId'\n",
    "    POSITION = 'Position'\n",
    "    START_TIME = 'StartTime'\n",
    "    END_TIME = 'EndTime'\n",
    "    BOOKED = 'Booked'\n",
    "    orders_booked = spark.createDataFrame(orders.filter('Closed = False').collect()[:n_count_order])\n",
    "    ids_drivers = drivers.select(drivers.Id).join(orders_booked, orders_booked.DriverId==drivers.Id, 'left').where('DriverId is null')\n",
    "    ids_drivers = ids_drivers.select(drivers.Id).collect()[:n_count_worde]\n",
    "    \n",
    "    ids_drivers_booked = orders_booked.select('DriverId').collect()\n",
    "    coordinates = london_address.select('oseast1m', 'osnrth1m').collect()\n",
    "    grids = [str({'oseast1m': int(i['oseast1m']),'osnrth1m': int(i['osnrth1m'])}) for i in coordinates]\n",
    "    \n",
    "    id_start = 0\n",
    "    \n",
    "    if movement != None:\n",
    "        id_start = movement.agg({'id': 'max'}).collect()[0]['max(id)'] + 1\n",
    "    \n",
    "    rows = []\n",
    "    for i in ids_drivers_booked:\n",
    "        driverId = i['DriverId']\n",
    "        row = (id_start, driverId, None, None, None, True)\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "        \n",
    "    for i in ids_drivers:\n",
    "        driverId = i['Id']\n",
    "        row = (id_start, driverId, None, None, None, False)\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "    \n",
    "    df_movement = spark.createDataFrame(rows, schema='Id bigint, DriverId bigint, Position string, StartTime timestamp, EndTime timestamp, Booked boolean')\n",
    "    rows = []\n",
    "    n = n_count_order + n_count_worde\n",
    "    delta = dt.timedelta(minutes=30)\n",
    "    times = [{'start': datetime.utcnow(), 'end': datetime.utcnow() + delta} for i in range(n)]\n",
    "    k = 0\n",
    "    for i in range(100):\n",
    "        candidates = random.choices(grids, k=n)\n",
    "        \n",
    "        for j in range(n_count_order):\n",
    "            id_driver = ids_drivers_booked[j]['DriverId']\n",
    "            position = candidates[j]\n",
    "            order = orders_booked.filter(orders_booked.DriverId == id_driver).collect()[0]\n",
    "            movement = df_movement.filter(df_movement.DriverId == id_driver).collect()[0]\n",
    "            if (order['End'] == position):\n",
    "                rows.append((movement[ID], movement[DRIVER_ID], position, times[j]['start'], times[j]['end'], False))\n",
    "            else:\n",
    "                rows.append((movement[ID], movement[DRIVER_ID], position, times[j]['start'], None, True))\n",
    "            \n",
    "            if i == 99:\n",
    "                rows.append((movement[ID], movement[DRIVER_ID], position, times[j]['end'], times[j]['end'], False))\n",
    "                \n",
    "        for j in range(n_count_order, n):\n",
    "            id_driver = ids_drivers[j-n_count_order]['Id']\n",
    "            position = candidates[j]\n",
    "            movement = df_movement.filter(df_movement.DriverId == id_driver).collect()[0]\n",
    "            rows.append((movement[ID], movement[DRIVER_ID], position, movement[START_TIME], movement[END_TIME], movement[BOOKED]))\n",
    "        \n",
    "                    \n",
    "        if k % 100 == 0:\n",
    "            print(k)\n",
    "            print(candidates)\n",
    "\n",
    "        k += 1\n",
    "    df = spark.createDataFrame(rows, schema='Id bigint, DriverId bigint, Position string, StartTime timestamp, EndTime timestamp, Booked boolean')\n",
    "    return df, df_movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[\"{'oseast1m': 523167, 'osnrth1m': 189445}\", \"{'oseast1m': 531739, 'osnrth1m': 172489}\", \"{'oseast1m': 543760, 'osnrth1m': 186389}\", \"{'oseast1m': 530647, 'osnrth1m': 176164}\", \"{'oseast1m': 535483, 'osnrth1m': 195078}\", \"{'oseast1m': 533985, 'osnrth1m': 166601}\", \"{'oseast1m': 517054, 'osnrth1m': 177215}\", \"{'oseast1m': 521584, 'osnrth1m': 188555}\", \"{'oseast1m': 536713, 'osnrth1m': 178558}\", \"{'oseast1m': 526041, 'osnrth1m': 183910}\"]\n"
     ]
    }
   ],
   "source": [
    "df, df_movement = simulate_movement(drivers, orders_back, london_address, None, n_count_order=5, n_count_worde=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_back = spark.read.parquet('spark/orders_backup.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client.csv\tdriver.parquet\t\t movement.parquet\r\n",
      "client.parquet\tfeedback.parquet\t orders_backup.parquet\r\n",
      "data.zip\tlondon-address.csv\t orders.parquet\r\n",
      "driver.csv\tmovement_backup.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_new = df.unionAll(df_movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = movement_new.filter('Booked==False and EndTime is not null').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=0, DriverId=196537, Position=\"{'oseast1m': 547219, 'osnrth1m': 172384}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26373), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26373), Booked=False),\n",
       " Row(Id=1, DriverId=48763, Position=\"{'oseast1m': 538250, 'osnrth1m': 165993}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26374), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26374), Booked=False),\n",
       " Row(Id=2, DriverId=335348, Position=\"{'oseast1m': 541967, 'osnrth1m': 185651}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), Booked=False),\n",
       " Row(Id=3, DriverId=109440, Position=\"{'oseast1m': 531534, 'osnrth1m': 182589}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), Booked=False),\n",
       " Row(Id=4, DriverId=120362, Position=\"{'oseast1m': 528487, 'osnrth1m': 164905}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26376), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26376), Booked=False)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders = orders_back.withColumn('Closed', F.when(F.col('DriverId').between(closed[0]['DriverId'],closed[0]['DriverId']),True).otherwise(F.col('Closed')))\n",
    "new_orders = new_orders.withColumn('Modified', F.when(F.col('DriverId').between(closed[0]['DriverId'],closed[0]['DriverId']),datetime.utcnow()).otherwise(F.col('Modified')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(closed)):\n",
    "    new_orders = new_orders.withColumn('Closed', F.when(F.col('DriverId').between(closed[i]['DriverId'],closed[i]['DriverId']),True).otherwise(F.col('Closed')))\n",
    "    new_orders = new_orders.withColumn('Modified', F.when(F.col('DriverId').between(closed[i]['DriverId'],closed[i]['DriverId']),datetime.utcnow()).otherwise(F.col('Modified')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=0, DriverId=196537, Position=\"{'oseast1m': 547219, 'osnrth1m': 172384}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26373), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26373), Booked=False),\n",
       " Row(Id=1, DriverId=48763, Position=\"{'oseast1m': 538250, 'osnrth1m': 165993}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26374), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26374), Booked=False),\n",
       " Row(Id=2, DriverId=335348, Position=\"{'oseast1m': 541967, 'osnrth1m': 185651}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), Booked=False),\n",
       " Row(Id=3, DriverId=109440, Position=\"{'oseast1m': 531534, 'osnrth1m': 182589}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26375), Booked=False),\n",
       " Row(Id=4, DriverId=120362, Position=\"{'oseast1m': 528487, 'osnrth1m': 164905}\", StartTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26376), EndTime=datetime.datetime(2021, 12, 16, 3, 58, 55, 26376), Booked=False)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------+--------+-----+------+--------------------+--------------------+\n",
      "|    Id|               Start|                 End|ClientId|DriverId|Price|Closed|             Created|            Modified|\n",
      "+------+--------------------+--------------------+--------+--------+-----+------+--------------------+--------------------+\n",
      "|187392|{'oseast1m': 5286...|{'oseast1m': 5336...|   85645|  196537|  673|  true|2021-12-15 21:22:...|2021-12-16 03:47:...|\n",
      "|187393|{'oseast1m': 5389...|{'oseast1m': 5361...|  346039|   48763|  128|  true|2021-12-15 21:22:...|2021-12-16 03:47:...|\n",
      "|187394|{'oseast1m': 5316...|{'oseast1m': 5165...|  914349|  335348| 1279|  true|2021-12-15 21:22:...|2021-12-16 03:47:...|\n",
      "|187395|{'oseast1m': 5325...|{'oseast1m': 5246...|  895537|  109440| 1155|  true|2021-12-15 21:22:...|2021-12-16 03:47:...|\n",
      "|187396|{'oseast1m': 5197...|{'oseast1m': 5251...|  895033|  120362|  921|  true|2021-12-15 21:22:...|2021-12-16 03:47:...|\n",
      "+------+--------------------+--------------------+--------+--------+-----+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_orders.filter('Closed == True').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders.write.parquet('spark/orders.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+-------+------+\n",
      "| Id|DriverId|            Position|           StartTime|EndTime|Booked|\n",
      "+---+--------+--------------------+--------------------+-------+------+\n",
      "|  0|  196537|{'oseast1m': 5231...|2021-12-16 03:28:...|   null|  true|\n",
      "|  1|   48763|{'oseast1m': 5317...|2021-12-16 03:28:...|   null|  true|\n",
      "|  2|  335348|{'oseast1m': 5437...|2021-12-16 03:28:...|   null|  true|\n",
      "|  3|  109440|{'oseast1m': 5306...|2021-12-16 03:28:...|   null|  true|\n",
      "|  4|  120362|{'oseast1m': 5354...|2021-12-16 03:28:...|   null|  true|\n",
      "|  5|       0|{'oseast1m': 5339...|                null|   null| false|\n",
      "|  6|       1|{'oseast1m': 5170...|                null|   null| false|\n",
      "|  7|       5|{'oseast1m': 5215...|                null|   null| false|\n",
      "|  8|       6|{'oseast1m': 5367...|                null|   null| false|\n",
      "|  9|       7|{'oseast1m': 5260...|                null|   null| false|\n",
      "|  0|  196537|{'oseast1m': 5329...|2021-12-16 03:28:...|   null|  true|\n",
      "|  1|   48763|{'oseast1m': 5210...|2021-12-16 03:28:...|   null|  true|\n",
      "|  2|  335348|{'oseast1m': 5178...|2021-12-16 03:28:...|   null|  true|\n",
      "|  3|  109440|{'oseast1m': 5158...|2021-12-16 03:28:...|   null|  true|\n",
      "|  4|  120362|{'oseast1m': 5331...|2021-12-16 03:28:...|   null|  true|\n",
      "|  5|       0|{'oseast1m': 5257...|                null|   null| false|\n",
      "|  6|       1|{'oseast1m': 5261...|                null|   null| false|\n",
      "|  7|       5|{'oseast1m': 5285...|                null|   null| false|\n",
      "|  8|       6|{'oseast1m': 5212...|                null|   null| false|\n",
      "|  9|       7|{'oseast1m': 5310...|                null|   null| false|\n",
      "+---+--------+--------------------+--------------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movement_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_new.write.parquet('spark/movement.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement = spark.read.parquet('spark/movement.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+--------------------+--------------------+------+\n",
      "| Id|DriverId|            Position|           StartTime|             EndTime|Booked|\n",
      "+---+--------+--------------------+--------------------+--------------------+------+\n",
      "|  0|  196537|{'oseast1m': 5472...|2021-12-16 03:58:...|2021-12-16 03:58:...| false|\n",
      "|  1|   48763|{'oseast1m': 5382...|2021-12-16 03:58:...|2021-12-16 03:58:...| false|\n",
      "|  2|  335348|{'oseast1m': 5419...|2021-12-16 03:58:...|2021-12-16 03:58:...| false|\n",
      "|  3|  109440|{'oseast1m': 5315...|2021-12-16 03:58:...|2021-12-16 03:58:...| false|\n",
      "|  4|  120362|{'oseast1m': 5284...|2021-12-16 03:58:...|2021-12-16 03:58:...| false|\n",
      "+---+--------+--------------------+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movement.where('Booked == False and StartTime is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feedback(orders, delta, feedback=None):\n",
    "\n",
    "    id_start = 0\n",
    "    \n",
    "    if movement != None:\n",
    "        id_start = movement.agg({'id': 'max'}).collect()[0]['max(id)'] + 1\n",
    "\n",
    "    orders_closed = orders.filter('Closed == True').filter(F.to_utc_timestamp(F.current_timestamp(), tz='+02').cast('long') - F.col('Modified').cast('long') < delta).collect()\n",
    "    \n",
    "    rows = []\n",
    "    for order in orders_closed:\n",
    "        raitingClient = random.randint(1,6)\n",
    "        raitingDriver = random.randint(1,6)\n",
    "        messageClient = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        messageDriver = ''.join(random.choices(string.ascii_letters + string.digits, k=random.randint(5,25)))\n",
    "        row = (id_start, order['DriverId'], order['Id'], order['ClientId'], raitingClient, raitingDriver, messageDriver, messageClient)\n",
    "        rows.append(row)\n",
    "        id_start += 1\n",
    "    \n",
    "    df = spark.createDataFrame(rows, schema='Id bigint, DriverId bigint, OrderId bigint, ClientId bigint, RaitingClient int, RaitingDriver int, MessageDriver string, MessageClient string')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = create_feedback(new_orders, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback.write.parquet('spark/feedback.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = spark.read.parquet('spark/feedback.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|DriverId|avg(Raiting)|\n",
      "+--------+------------+\n",
      "|  335348|         2.0|\n",
      "|   48763|         2.0|\n",
      "|  109440|         3.0|\n",
      "|  120362|         1.0|\n",
      "|  196537|         3.0|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedback.select('DriverId', F.col('RaitingDriver').alias('Raiting')).groupBy('DriverId').agg({'Raiting': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
